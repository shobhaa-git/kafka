/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package kafka.log.remote

import kafka.cluster.Partition
import kafka.log.{AbortedTxn, Log, OffsetPosition}
import kafka.metrics.KafkaMetricsGroup
import kafka.server._
import kafka.server.checkpoints.{CheckpointWriteBuffer, LeaderEpochCheckpoint, LeaderEpochCheckpointFile}
import kafka.server.epoch.{EpochEntry, LeaderEpochFileCache}
import kafka.utils.Logging
import org.apache.kafka.clients.CommonClientConfigs
import org.apache.kafka.common._
import org.apache.kafka.common.errors.OffsetOutOfRangeException
import org.apache.kafka.common.internals.Topic
import org.apache.kafka.common.record.FileRecords.TimestampAndOffset
import org.apache.kafka.common.record.{MemoryRecords, RecordBatch, RemoteLogInputStream}
import org.apache.kafka.common.requests.FetchRequest.PartitionData
import org.apache.kafka.common.requests.FetchResponse.AbortedTransaction
import org.apache.kafka.common.utils.{ChildFirstClassLoader, KafkaThread, Time, Utils}
import org.apache.kafka.server.log.remote.metadata.storage.{ClassLoaderAwareRemoteLogMetadataManager, TopicBasedRemoteLogMetadataManagerConfig}
import org.apache.kafka.server.log.remote.storage._
import java.io.{BufferedWriter, ByteArrayOutputStream, Closeable, InputStream, OutputStreamWriter}
import java.nio.ByteBuffer
import java.nio.charset.StandardCharsets
import java.security.{AccessController, PrivilegedAction}
import java.util.Optional
import java.util.concurrent._
import java.util.concurrent.atomic.AtomicInteger
import java.{lang, util}

import scala.collection.Searching._
import scala.collection.mutable.ListBuffer
import scala.collection.{Seq, Set}
import scala.collection.concurrent
import scala.jdk.CollectionConverters._

class RLMScheduledThreadPool(poolSize: Int) extends Logging {

  private val scheduledThreadPool: ScheduledThreadPoolExecutor = {
    val threadPool = new ScheduledThreadPoolExecutor(poolSize)
    threadPool.setRemoveOnCancelPolicy(true)
    threadPool.setExecuteExistingDelayedTasksAfterShutdownPolicy(false)
    threadPool.setThreadFactory(new ThreadFactory {
      private val sequence = new AtomicInteger()

      override def newThread(r: Runnable): Thread = {
        KafkaThread.daemon("kafka-rlm-thread-pool-" + sequence.incrementAndGet(), r)
      }
    })

    threadPool
  }

  def resizePool(size: Int): Unit = {
    info(s"Resizing pool from ${scheduledThreadPool.getCorePoolSize} to $size")
    scheduledThreadPool.setCorePoolSize(size)
  }

  def poolSize(): Int = scheduledThreadPool.getMaximumPoolSize

  def getIdlePercent(): Double = {
    1 - scheduledThreadPool.getActiveCount().asInstanceOf[Double] / scheduledThreadPool.getCorePoolSize.asInstanceOf[Double]
  }

  def scheduleWithFixedDelay(runnable: Runnable, initialDelay: Long, delay: Long,
                             timeUnit: TimeUnit): ScheduledFuture[_] = {
    info(s"Scheduling runnable $runnable with initial delay: $initialDelay, fixed delay: $delay")
    scheduledThreadPool.scheduleWithFixedDelay(runnable, initialDelay, delay, timeUnit)
  }

  def shutdown(): Boolean = {
    info("Shutting down scheduled thread pool")
    scheduledThreadPool.shutdownNow()
    //waits for 2 mins to terminate the current tasks
    scheduledThreadPool.awaitTermination(2, TimeUnit.MINUTES)
  }
}

trait CancellableRunnable extends Runnable {
  @volatile private var cancelled = false

  def cancel(): Unit = {
    cancelled = true
  }

  def isCancelled(): Boolean = {
    cancelled
  }
}

class RemoteLogManager(fetchLog: TopicPartition => Option[Log],
                       updateRemoteLogStartOffset: (TopicPartition, Long) => Unit,
                       rlmConfig: RemoteLogManagerConfig,
                       time: Time = Time.SYSTEM,
                       brokerId: Int,
                       clusterId: String = "",
                       logDir: String,
                       brokerTopicStats: BrokerTopicStats) extends Logging with Closeable with KafkaMetricsGroup  {
  private val leaderOrFollowerTasks: ConcurrentHashMap[TopicIdPartition, RLMTaskWithFuture] =
    new ConcurrentHashMap[TopicIdPartition, RLMTaskWithFuture]()
  private val remoteStorageFetcherThreadPool = new RemoteStorageReaderThreadPool(rlmConfig.remoteLogReaderThreads,
    rlmConfig.remoteLogReaderMaxPendingTasks, time)

  private val delayInMs = rlmConfig.remoteLogManagerTaskIntervalMs
  private val poolSize = rlmConfig.remoteLogManagerThreadPoolSize
  private val rlmScheduledThreadPool = new RLMScheduledThreadPool(poolSize)

  private val onLeadershipChangeLock = new Object

  // topic ids that are received on leadership changes, this map is cleared on stop partitions
  private val topicPartitionIds: concurrent.Map[TopicPartition, Uuid] = new ConcurrentHashMap[TopicPartition, Uuid]().asScala


  @volatile private var closed = false

  newGauge("RemoteLogManagerTasksAvgIdlePercent", () => {
    rlmScheduledThreadPool.getIdlePercent()
  })

  private[remote] def createRemoteStorageManager(): ClassLoaderAwareRemoteStorageManager = {
    AccessController.doPrivileged(new PrivilegedAction[ClassLoaderAwareRemoteStorageManager] {
      private val classPath = rlmConfig.remoteStorageManagerClassPath()

      override def run(): ClassLoaderAwareRemoteStorageManager = {
        val classLoader =
          if (classPath != null && classPath.trim.nonEmpty) {
            new ChildFirstClassLoader(classPath, this.getClass.getClassLoader)
          } else {
            this.getClass.getClassLoader
          }
        val delegate = classLoader.loadClass(rlmConfig.remoteStorageManagerClassName())
          .getDeclaredConstructor().newInstance().asInstanceOf[RemoteStorageManager]
        new ClassLoaderAwareRemoteStorageManager(delegate, classLoader)
      }
    })
  }

  private def configureRSM(): Unit = {
    val rsmProps = new util.HashMap[String, Any]()
    rlmConfig.remoteStorageManagerProps().asScala.foreach { case (k, v) => rsmProps.put(k, v) }
    rsmProps.put(KafkaConfig.BrokerIdProp, brokerId)
    rsmProps.put("cluster.id", clusterId)
    remoteLogStorageManager.configure(rsmProps)
  }

  private[remote] def createRemoteLogMetadataManager(): RemoteLogMetadataManager = {
    AccessController.doPrivileged(new PrivilegedAction[RemoteLogMetadataManager] {
      private val classPath = rlmConfig.remoteLogMetadataManagerClassPath

      override def run(): RemoteLogMetadataManager = {
        var classLoader = this.getClass.getClassLoader
        if (classPath != null && classPath.trim.nonEmpty) {
          classLoader = new ChildFirstClassLoader(classPath, classLoader)
          val delegate = classLoader.loadClass(rlmConfig.remoteLogMetadataManagerClassName())
            .getDeclaredConstructor()
            .newInstance()
            .asInstanceOf[RemoteLogMetadataManager]
          new ClassLoaderAwareRemoteLogMetadataManager(delegate, classLoader)
        } else {
          classLoader.loadClass(rlmConfig.remoteLogMetadataManagerClassName())
            .getDeclaredConstructor()
            .newInstance()
            .asInstanceOf[RemoteLogMetadataManager]
        }
      }
    })
  }

  private def configureRLMM(endPoint: Endpoint): Unit = {
    val rlmmProps = new util.HashMap[String, Any]()
    rlmConfig.remoteLogMetadataManagerProps().asScala.foreach { case (k, v) => rlmmProps.put(k, v) }
    rlmmProps.put(KafkaConfig.LogDirProp, logDir)
    rlmmProps.put(KafkaConfig.BrokerIdProp, brokerId)
    rlmmProps.put("cluster.id", clusterId)
    rlmmProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, endPoint.host + ":" + endPoint.port)
    rlmmProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, endPoint.securityProtocol.name)
    remoteLogMetadataManager.configure(rlmmProps)
  }

  private val remoteLogStorageManager: ClassLoaderAwareRemoteStorageManager = createRemoteStorageManager()
  val remoteLogMetadataManager: RemoteLogMetadataManager = createRemoteLogMetadataManager()

  private val indexCache = new RemoteIndexCache(remoteStorageManager = remoteLogStorageManager, logDir = logDir)

  private[remote] def rlmScheduledThreadPoolSize: Int = rlmScheduledThreadPool.poolSize()

  private[remote] def leaderOrFollowerTasksSize: Int = leaderOrFollowerTasks.size()

  private def doHandleLeaderOrFollowerPartitions(topicPartition: TopicIdPartition,
                                                 convertToLeaderOrFollower: RLMTask => Unit): Unit = {
    val rlmTaskWithFuture = leaderOrFollowerTasks.computeIfAbsent(topicPartition, (tp: TopicIdPartition) => {
      val task = new RLMTask(tp)
      // set this upfront when it is getting initialized instead of doing it after scheduling.
      convertToLeaderOrFollower(task)
      info(s"Created a new task: $task and getting scheduled")
      val future = rlmScheduledThreadPool.scheduleWithFixedDelay(task, 0, delayInMs, TimeUnit.MILLISECONDS)
      RLMTaskWithFuture(task, future)
    })
    convertToLeaderOrFollower(rlmTaskWithFuture.rlmTask)
  }

  def onEndpointCreated(serverEndPoint: Endpoint): Unit = {
    // initialize and configure RSM and RLMM
    configureRSM()
    configureRLMM(serverEndPoint)
  }

  def storageManager(): RemoteStorageManager = {
    remoteLogStorageManager
  }

  /**
   * Callback to receive any leadership changes for the topic partitions assigned to this broker. If there are no
   * existing tasks for a given topic partition then it will assign new leader or follower task else it will convert the
   * task to respective target state(leader or follower).
   *
   * @param partitionsBecomeLeader   partitions that have become leaders on this broker.
   * @param partitionsBecomeFollower partitions that have become followers on this broker.
   */
  def onLeadershipChange(partitionsBecomeLeader: Set[Partition],
                         partitionsBecomeFollower: Set[Partition],
                         topicIds: util.Map[String, Uuid]): Unit = {
    trace(s"Received leadership changes for partitionsBecomeLeader: $partitionsBecomeLeader " +
      s"and partitionsBecomeLeader: $partitionsBecomeLeader")
    // Compact topics and internal topics are filtered here as they are not supported with tiered storage.
    def nonSupported(partition: Partition): Boolean = {
      Topic.isInternal(partition.topic) ||
        // fetchLog should reflect up to date configuration
        fetchLog(partition.topicPartition).exists(log => log.config.compact || !log.config.remoteStorageEnable) ||
        partition.topicPartition.topic().equals(TopicBasedRemoteLogMetadataManagerConfig.REMOTE_LOG_METADATA_TOPIC_NAME)
    }

    def cacheTopicPartitionIds(topicIdPartition: TopicIdPartition): Unit = {
      val maybePreviousTopicId = topicPartitionIds.put(topicIdPartition.topicPartition(), topicIdPartition.topicId())
      maybePreviousTopicId.filter(_ != topicIdPartition.topicId())
        .foreach { previousTopicId =>
          warn(s"Previous cached topic id $previousTopicId for ${topicIdPartition.topicPartition()} does " +
            s"not match update topic id ${topicIdPartition.topicId()}")
        }
    }

    /**
     * The following scenarios are possible when the onLeadershipChange method is invoked:
     *
     * 1. LeaderAndIsrRequest arrives before ConfigHandler config change for remoteStorageEnable: In this case, there will be a separate call for config change.
     * 2. LeaderAndIsrRequest arrives after ConfigHandler config for remoteStorageEnable: In this case, the config changes will not be lost as there is a fetchLog call now to fetch the log updates.
     * 3. LeaderAndIsrRequest arrives at same time as ConfigHandler config for remoteStorageEnable: Falls in to 1 or 2 above depending on sequencing
     *
     * This method is safe for all the three scenarios and will be able to pick up the updated configuration.
     */
    onLeadershipChangeLock synchronized {
      val leaderPartitions = partitionsBecomeLeader.filterNot(nonSupported)
        .map(p => new TopicIdPartition(topicIds.get(p.topic), p.topicPartition) -> p).toMap
      val followerPartitions = partitionsBecomeFollower.filterNot(nonSupported)
        .map(p => new TopicIdPartition(topicIds.get(p.topic), p.topicPartition))

      if (leaderPartitions.nonEmpty || followerPartitions.nonEmpty) {
        debug(s"Effective topic partitions after filtering compact and internal topics, " +
          s"leaders: ${leaderPartitions.keySet} and followers: $followerPartitions")

        leaderPartitions.keySet.foreach(cacheTopicPartitionIds)
        followerPartitions.foreach(cacheTopicPartitionIds)

        remoteLogMetadataManager.onPartitionLeadershipChanges(leaderPartitions.keySet.asJava, followerPartitions.asJava)
        followerPartitions.foreach {
          topicIdPartition => doHandleLeaderOrFollowerPartitions(topicIdPartition, _.convertToFollower())
        }
        leaderPartitions.foreach {
          case (topicIdPartition, partition) =>
            doHandleLeaderOrFollowerPartitions(topicIdPartition, _.convertToLeader(partition.getLeaderEpoch))
        }
      }
    }
    None
  }

  /**
   * Stops partitions for copying segments, building indexes.
   *
   * @param allPartitions  topic partitions that needs to be stopped.
   * @param delete      flag to indicate whether the given topic partitions to be deleted or not.
   */
  def stopPartitions(allPartitions: Set[TopicPartition], delete: Boolean, errorHandler: (TopicPartition, Throwable) => Unit): Unit = {
    debug(s"Stopping ${allPartitions.size} partitions, delete: $delete")

    val tpIds = allPartitions
      .filter(topicPartitionIds contains)
      .map(tp => new TopicIdPartition(topicPartitionIds(tp), tp))
      .toSet

    tpIds.foreach(tpId => {
      val topicPartition = tpId.topicPartition()
      try {
        val task = leaderOrFollowerTasks.remove(tpId)
        if (task != null) {
          info(s"Cancelling the RLM task for tp: $topicPartition")
          task.cancel()
        }
        if (delete) {
          info(s"Deleting the remote log segments for partition: $tpId")
          remoteLogMetadataManager.listRemoteLogSegments(tpId).forEachRemaining(elt => deleteRemoteLogSegment(elt, _ => true))
        }

        brokerTopicStats.partitionAggregatedStats(topicPartition.topic()).removePartition(topicPartition.partition())

      } catch {
        case ex: Throwable => errorHandler(topicPartition, ex)
      }
    })

    if (delete) {
      // NOTE: this#stopPartitions method is called when Replica state changes to Offline and ReplicaDeletionStarted
      remoteLogMetadataManager.onStopPartitions(tpIds.asJava)
      topicPartitionIds --= allPartitions
      indexCache.removeIndexEntriesForDeletedTopicIdPartitions(tpIds)
    }
  }

  private def deleteRemoteLogSegment(segmentMetadata: RemoteLogSegmentMetadata, predicate: RemoteLogSegmentMetadata => Boolean): Boolean = {
    if (predicate(segmentMetadata)) {
      // Publish delete segment started event.
      remoteLogMetadataManager.updateRemoteLogSegmentMetadata(
        new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(),
          RemoteLogSegmentState.DELETE_SEGMENT_STARTED, brokerId))

      // Delete the segment in remote storage.
      remoteLogStorageManager.deleteLogSegmentData(segmentMetadata)

      // Publish delete segment finished event.
      remoteLogMetadataManager.updateRemoteLogSegmentMetadata(
        new RemoteLogSegmentMetadataUpdate(segmentMetadata.remoteLogSegmentId(), time.milliseconds(),
          RemoteLogSegmentState.DELETE_SEGMENT_FINISHED, brokerId))
      true
    } else false
  }

  /**
   * Returns the leader epoch checkpoint by truncating with the given start[exclusive] and end[inclusive] offset
   * @param log         The actual log from where to take the leader-epoch checkpoint
   * @param startOffset The start offset of the checkpoint file (exclusive in the truncation).
   *                    If start offset is 6, then it will retain an entry at offset 6.
   * @param endOffset   The end offset of the checkpoint file (inclusive in the truncation)
   *                    If end offset is 100, then it will remove the entries greater than or equal to 100.
   * @return the truncated leader epoch checkpoint
   */
  private[remote] def getLeaderEpochCheckpoint(log: Log, startOffset: Long, endOffset: Long): InMemoryLeaderEpochCheckpoint = {
    val checkpoint = new InMemoryLeaderEpochCheckpoint()
    log.leaderEpochCache
      .map(cache => cache.writeTo(checkpoint))
      .foreach { x =>
        if (startOffset >= 0) {
          x.truncateFromStart(startOffset)
        }
        x.truncateFromEnd(endOffset)
      }
    checkpoint
  }

  private[remote] class RLMTask(tpId: TopicIdPartition) extends CancellableRunnable with Logging {
    this.logIdent = s"[RemoteLogManager=$brokerId partition=$tpId] "
    @volatile private var leaderEpoch: Int = -1

    private def isLeader(): Boolean = leaderEpoch >= 0

    // The readOffset is None initially for a new leader RLMTask,
    // and needs to be fetched inside the task's run() method.
    private var readOffsetOption: Option[Long] = None

    private val remoteLogAggregateStatsCache: RemoteLogAggregateStatsCache =
      new RemoteLogAggregateStatsCache(brokerId, tpId, time)

    //todo-updating log with remote index highest offset -- should this be required?
    // fetchLog(tp.topicPartition()).foreach { log => log.updateRemoteIndexHighestOffset(readOffset) }

    def convertToLeader(leaderEpochVal: Int): Unit = {
      if (leaderEpochVal < 0) {
        throw new KafkaException(s"leaderEpoch value for topic partition $tpId can not be negative")
      }
      if (this.leaderEpoch != leaderEpochVal) {
        leaderEpoch = leaderEpochVal
      }
      // Reset readOffset, so that it is set in next run of RLMTask
      readOffsetOption = None
    }

    def convertToFollower(): Unit = {
      leaderEpoch = -1
      remoteLogAggregateStatsCache.clear()
    }

    def copyLogSegmentsToRemote(): Unit = {
      if (isCancelled())
        return

      def maybeUpdateReadOffset(): Unit = {
        if (readOffsetOption.isEmpty) {
          info(s"Find the highest remote offset for partition: $tpId after becoming leader, leaderEpoch: $leaderEpoch")

          // This is found by traversing from the latest leader epoch from leader epoch history and find the highest offset
          // of a segment with that epoch copied into remote storage. If it can not find an entry then it checks for the
          // previous leader epoch till it finds an entry, If there are no entries till the earliest leader epoch in leader
          // epoch cache then it starts copying the segments from the earliest epoch entryâ€™s offset.
          readOffsetOption = Some(findHighestRemoteOffset(tpId))
        }
      }

      try {
        maybeUpdateReadOffset()
        val readOffset = readOffsetOption.get
        fetchLog(tpId.topicPartition()).foreach { log =>
          // LSO indicates the offset below are ready to be consumed(high-watermark or committed)
          val lso = log.lastStableOffset
          if (lso < 0) {
            warn(s"lastStableOffset for partition $tpId is $lso, which should not be negative.")
          } else if (lso > 0 && readOffset < lso) {
            // copy segments only till the min of high-watermark or stable-offset
            // remote storage should contain only committed/acked messages
            val fetchOffset = lso
            debug(s"Checking for segments to copy, readOffset: $readOffset and fetchOffset: $fetchOffset")
            val activeSegBaseOffset = log.activeSegment.baseOffset
            // log-start-offset can be ahead of the read-offset, when:
            // 1) log-start-offset gets incremented via delete-records API (or)
            // 2) enabling the remote log for the first time, the log-start-offset can be ahead of the local-log base-segment-offset due to segment deletion.
            val fromOffset = Math.max(readOffset + 1, log.logStartOffset)
            val sortedSegments = log.logSegments(fromOffset, fetchOffset).toSeq.sortBy(_.baseOffset)
            val index: Int = sortedSegments.map(x => x.baseOffset).search(activeSegBaseOffset) match {
              case Found(x) => x
              case InsertionPoint(y) => y - 1
            }
            if (index < 0) {
              debug(s"No segments found to be copied for partition $tpId with read offset: $readOffset and active " +
                s"baseoffset: $activeSegBaseOffset")
            } else {
              sortedSegments.slice(0, index).foreach { segment =>
                // store locally here as this may get updated after the below if condition is computed as false.
                if (isCancelled() || !isLeader()) {
                  info(s"Skipping copying log segments as the current task state is changed, cancelled: " +
                    s"${isCancelled()} leader:${isLeader()}")
                  return
                }

                val logFile = segment.log.file()
                val fileName = logFile.getName
                info(s"Copying $fileName to remote storage.")
                val id = new RemoteLogSegmentId(tpId, Uuid.randomUuid())

                val nextOffset = segment.readNextOffset
                //todo-tier double check on this
                val endOffset = nextOffset - 1
                val producerIdSnapshotFile = log.producerStateManager.fetchSnapshot(nextOffset).orNull

                val segmentLeaderEpochs = getLeaderEpochCheckpoint(log, segment.baseOffset, nextOffset).read().map {
                  case EpochEntry(epoch, startOffset) => Integer.valueOf(epoch) -> lang.Long.valueOf(startOffset)
                }.toMap.asJava

                val remoteLogSegmentMetadata = new RemoteLogSegmentMetadata(id, segment.baseOffset, endOffset,
                  segment.largestTimestamp, brokerId, time.milliseconds(), segment.log.sizeInBytes(),
                  segmentLeaderEpochs)

                remoteLogMetadataManager.addRemoteLogSegmentMetadata(remoteLogSegmentMetadata)

                val leaderEpochsIndex = getLeaderEpochCheckpoint(log, startOffset = -1, nextOffset).readAsByteBuffer()
                val segmentData = new LogSegmentData(logFile.toPath, segment.lazyOffsetIndex.get.path,
                  segment.lazyTimeIndex.get.path, Optional.ofNullable(segment.txnIndex.path),
                  producerIdSnapshotFile.toPath, leaderEpochsIndex)
                remoteLogStorageManager.copyLogSegmentData(remoteLogSegmentMetadata, segmentData)

                val rlsmAfterCreate = new RemoteLogSegmentMetadataUpdate(id, time.milliseconds(),
                  RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId)

                remoteLogMetadataManager.updateRemoteLogSegmentMetadata(rlsmAfterCreate)
                remoteLogAggregateStatsCache.add(remoteLogSegmentMetadata.segmentSizeInBytes())
                brokerTopicStats.topicStats(tpId.topicPartition().topic())
                  .remoteBytesOutRate.mark(remoteLogSegmentMetadata.segmentSizeInBytes())
                brokerTopicStats.allTopicsStats.remoteBytesOutRate.mark(remoteLogSegmentMetadata.segmentSizeInBytes())

                //todo-tier-storage
                log.updateRemoteIndexHighestOffset(endOffset)
                info(s"Copied $fileName to remote storage with segment-id: ${rlsmAfterCreate.remoteLogSegmentId()}")

                readOffsetOption = Some(endOffset)

                //
                // The tier lag of a topic-partition is defined as the number of records in non-active segments
                // not yet uploaded to the remote storage.
                //
                // The active segment base offset and remote log segment metadata end offset are both inclusive
                // (i.e. these offsets are included in their respective segments).
                //
                // Note that we use log.activeSegment.baseOffset rather than the previously assigned value
                // "activeSegBaseOffset" since the active segment before the segment upload may now have been
                // rolled over and replaced by a new segment.
                //
                val offsetLag = (log.activeSegment.baseOffset - 1) - endOffset
                val bytesLag = log.localOnlyLogSegmentsSize - log.activeSegment.size
                val (topic, partition) = (tpId.topicPartition().topic(), tpId.topicPartition().partition())
                brokerTopicStats.partitionAggregatedStats(topic).setLag(partition, offsetLag, bytesLag)
              }
            }
          } else {
            debug(s"Skipping copying segments, current read offset:$readOffset is and LSO:$lso ")
          }
        }
      } catch {
        case ex: Exception =>
          brokerTopicStats.topicStats(tpId.topicPartition().topic()).failedRemoteWriteRequestRate.mark()
          brokerTopicStats.allTopicsStats.failedRemoteWriteRequestRate.mark()
          if (!isCancelled()) {
            error(s"Error occurred while copying log segments of partition: $tpId", ex)
          }
      }
    }

    def handleExpiredRemoteLogSegments(): Unit = {
      if (isCancelled())
        return

      def handleLogStartOffsetUpdate(topicPartition: TopicPartition, remoteLogStartOffset: Long): Unit = {
        debug(s"Updating $topicPartition with remoteLogStartOffset: $remoteLogStartOffset")
        updateRemoteLogStartOffset(topicPartition, remoteLogStartOffset)
      }

      /*
      A segment has leader epochs (remote leader epochs). A partition leader has leader epochs (local leader epochs).
      For a segment to have a good lineage its leader epochs must be within the boundaries of the leader epochs
      of the partition leader.
      The first leader epoch in the segment always starts at the first offset of
      the segment even if the partition leader's leader epoch started at an earlier log offset.
      The last leader epoch in the segment always finishes at the last offset of
      the segment even if the partition leader's leader epoch finishes at a further log offset.

      CORRECT
      Remote:             i1 ===================== i2
      Local :        <=== j1 ======================================== j2 ===>

      CORRECT
      Remote:                                i1 ===================== i2
      Local :        <=== j1 ======================================== j2 ===>

      CORRECT
      Remote:                      i1 ===================== i2
      Local :        <=== j1 ======================================== j2 ===>

      The below is a situation where the remote segment has the same leader epoch as the currently
      ongoing local leader epoch.
      CORRECT
      Remote:                                i1 ===================== i2
      Local :        <=== j1 ===============================================>

      INCORRECT
      Remote:                                       i1 ===================== i2
      Local :        <=== j1 ======================================== j2 ===>

      INCORRECT
      Remote:   i1 ===================== i2
      Local :        <=== j1 ======================================== j2 ===>
      */
      def doesRemoteSegmentHaveGoodLineage(validEpochs: util.TreeMap[Integer, lang.Long],
                                           segmentMetadata: RemoteLogSegmentMetadata,
                                           logEndOffset: lang.Long): Boolean = {
        val segmentEpochs = segmentMetadata.segmentLeaderEpochs()
        val segmentEndOffset = segmentMetadata.endOffset()
        val firstSegmentEpochEntry = segmentEpochs.firstEntry()
        segmentEpochs.entrySet().stream()
          .allMatch(entry => {
            val segmentEpochKey = entry.getKey
            val segmentEpochStartOffset = entry.getValue
            if (validEpochs.containsKey(segmentEpochKey)) {
              val validEpochStartOffset = validEpochs.get(segmentEpochKey)
              val validEpochEndEntry = validEpochs.higherEntry(segmentEpochKey)
              val remoteEndEntry = segmentEpochs.higherEntry(segmentEpochKey)

              if (validEpochEndEntry == null && remoteEndEntry != null) {
                // Remote has more entries than local
                // This should be impossible but we are erring on the side of defining the behaviour
                val remoteLogSegmentId = segmentMetadata.remoteLogSegmentId()
                val topicIdPartition = segmentMetadata.topicIdPartition()
                val extraneousSegmentLeaderEpoch = remoteEndEntry.getKey
                warn(s"[Segment $remoteLogSegmentId; TopicPartition $topicIdPartition]: Segment has at least one more leader epoch ($extraneousSegmentLeaderEpoch) than locally ($segmentEpochKey)")
                false
              } else if (validEpochEndEntry != null && remoteEndEntry == null) {
                // Remote has fewer entries than local
                segmentEpochStartOffset >= validEpochStartOffset && segmentEndOffset < validEpochEndEntry.getValue
              } else if (validEpochEndEntry != null && remoteEndEntry != null) {
                // This is not the active leader epoch for both remote and local
                if (firstSegmentEpochEntry.getKey == segmentEpochKey) {
                  remoteEndEntry.getKey == validEpochEndEntry.getKey &&
                    segmentEpochStartOffset >= validEpochStartOffset &&
                    remoteEndEntry.getValue <= validEpochEndEntry.getValue
                } else {
                  remoteEndEntry.getKey == validEpochEndEntry.getKey &&
                    segmentEpochStartOffset == validEpochStartOffset &&
                    remoteEndEntry.getValue <= validEpochEndEntry.getValue
                }
              } else {
                // This is the active leader epoch
                segmentEndOffset < logEndOffset
              }
            } else {
              val remoteLogSegmentId = segmentMetadata.remoteLogSegmentId()
              val topicIdPartition = segmentMetadata.topicIdPartition()
              warn(s"[Segment $remoteLogSegmentId; TopicPartition $topicIdPartition]: Segment has a leader epoch ($segmentEpochKey) not found locally")
              false
            }
          })
      }

      def computeTotalLogSizeAndUpdateMetrics(log: Log): Long = {
        def computeRemoteLogAggregateStats(): RemoteLogAggregateStats = {
          val validLeaderEpochs = fromLeaderEpochCacheToEpochs(log)

          remoteLogMetadataManager.listRemoteLogSegments(tpId).asScala
            .filter(doesRemoteSegmentHaveGoodLineage(validLeaderEpochs, _, log.logEndOffset))
            .map(segment => RemoteLogAggregateStats(segment.segmentSizeInBytes(), 1))
            .reduceOption((a, b) => a.add(b))
            .getOrElse(RemoteLogAggregateStats(0, 0))
        }

        remoteLogAggregateStatsCache.getOrCompute(computeRemoteLogAggregateStats) match {
          case RemoteLogAggregateStats(remoteLogSizeBytes, numMetadataSegments) =>
            brokerTopicStats.partitionAggregatedStats(tpId.topicPartition().topic())
              .setRemoteLogAggregateStats(tpId.topicPartition().partition(), remoteLogSizeBytes, numMetadataSegments)

            remoteLogSizeBytes + log.localOnlyLogSegmentsSize
        }
      }

      def fromLeaderEpochCacheToEpochs(log: Log): util.TreeMap[Integer, lang.Long] = {
        log.leaderEpochCache.map(_.getEpochs).getOrElse(new util.TreeMap)
      }

      try {
        // cleanup remote log segments and update the log start offset if applicable.
        fetchLog(tpId.topicPartition()).foreach { log =>
          val retentionMs = log.config.retentionMs
          val (checkTimestampRetention, cleanupTs) = (retentionMs > -1, time.milliseconds() - retentionMs)

          val shouldDeleteBySize = log.config.retentionSize > -1
          val totalLogSize = computeTotalLogSizeAndUpdateMetrics(log)
          var remainingSize =
            if (shouldDeleteBySize) totalLogSize - log.config.retentionSize
            else 0

          var logStartOffset: Option[Long] = None

          def deleteRetentionTimeBreachedSegments(metadata: RemoteLogSegmentMetadata): Boolean = {
            val isSegmentDeleted = deleteRemoteLogSegment(
              metadata, checkTimestampRetention && _.maxTimestampMs() <= cleanupTs)
            if (isSegmentDeleted) {
              remainingSize = Math.max(0, remainingSize - metadata.segmentSizeInBytes())
              remoteLogAggregateStatsCache.subtract(metadata.segmentSizeInBytes())

              // It is fine to have logStartOffset as `metadata.endOffset() + 1` as the segment offset intervals
              // are ascending with in an epoch.
              logStartOffset = Some(metadata.endOffset() + 1)
              info(s"Deleted remote log segment ${metadata.remoteLogSegmentId()} due to retention time " +
                s"${retentionMs}ms breach based on the largest record timestamp in the segment")
            }
            isSegmentDeleted
          }

          def deleteRetentionSizeBreachedSegments(metadata: RemoteLogSegmentMetadata): Boolean = {
            val isSegmentDeleted = deleteRemoteLogSegment(metadata, metadata => {
              // Assumption that segments contain size > 0
                if (shouldDeleteBySize && remainingSize > 0) {
                remainingSize -= metadata.segmentSizeInBytes()
                remainingSize >= 0
              } else false
            })
            if (isSegmentDeleted) {
              remoteLogAggregateStatsCache.subtract(metadata.segmentSizeInBytes())

              logStartOffset = Some(metadata.endOffset() + 1)
              info(s"Deleted remote log segment ${metadata.remoteLogSegmentId()} due to retention size " +
                s"${log.config.retentionSize} breach. Log size after deletion will be " +
                s"${remainingSize + log.config.retentionSize}.")
            }
            isSegmentDeleted
          }

          val validLeaderEpochs = fromLeaderEpochCacheToEpochs(log)
          log.leaderEpochCache.foreach { cache =>
            cache.epochEntries.find { epochEntry =>
              val segmentsIterator =
                remoteLogMetadataManager.listRemoteLogSegments(tpId, epochEntry.epoch).asScala
                  .filter(doesRemoteSegmentHaveGoodLineage(validLeaderEpochs, _, log.logEndOffset))

              var isSegmentDeleted = true
              while (isSegmentDeleted && segmentsIterator.hasNext) {
                val metadata = segmentsIterator.next()
                isSegmentDeleted = deleteRetentionTimeBreachedSegments(metadata) ||
                  deleteRetentionSizeBreachedSegments(metadata)
              }
              !isSegmentDeleted
            }
          }
          logStartOffset.foreach(handleLogStartOffsetUpdate(tpId.topicPartition(), _))
        }
      } catch {
        case ex: Exception =>
          if (!isCancelled()) {
            error(s"Error while cleaning up log segments for partition: $tpId", ex)
          }
      }
    }

    override def run(): Unit = {
      if (isCancelled())
        return

      try {
        if (isLeader()) {
          // a. copy log segments to remote store
          copyLogSegmentsToRemote()

          // b. cleanup/delete expired remote segments
          // Followers will cleanup the local log cleanup based on the local logStartOffset.
          // We do not need any cleanup on followers from remote segments perspective.
          handleExpiredRemoteLogSegments()
        } else {
          fetchLog(tpId.topicPartition()).foreach { log =>
            val offset = findHighestRemoteOffset(tpId)
            log.updateRemoteIndexHighestOffset(offset)
          }
        }
      } catch {
        case ex: InterruptedException =>
          if (!isCancelled()) {
            warn(s"Current thread for topic-partition-id $tpId is interrupted, this task won't be rescheduled. " +
              s"Reason: ${ex.getMessage}")
          }
        case ex: Exception =>
          if (!isCancelled()) {
            warn(s"Current task for topic-partition $tpId received error but it will be scheduled. " +
              s"Reason: ${ex.getMessage}")
          }
      }
    }

    override def toString: String = {
      this.getClass.toString + s"[$tpId]"
    }
  }

  def findHighestRemoteOffset(topicIdPartition: TopicIdPartition): Long = {
    var offset: Optional[lang.Long] = Optional.empty()
    fetchLog(topicIdPartition.topicPartition()).foreach { log =>
      log.leaderEpochCache.foreach(cache => {
        var epoch = cache.latestEpoch
        while (!offset.isPresent && epoch.isDefined) {
          offset = remoteLogMetadataManager.highestOffsetForEpoch(topicIdPartition, epoch.get)
          epoch = cache.findPreviousEpoch(epoch.get)
        }
      })
    }
    offset.orElse(-1L)
  }

  def lookupPositionForOffset(remoteLogSegmentMetadata: RemoteLogSegmentMetadata, offset: Long): Int = {
    indexCache.lookupOffset(remoteLogSegmentMetadata, offset)
  }

  /*
   WARNING: Not all code-paths through the below function have been covered by unit tests.
   Ensure that if you modify something you add tests!
   */
  def read(remoteStorageFetchInfo: RemoteStorageFetchInfo): FetchDataInfo = {
    val fetchMaxBytes  = remoteStorageFetchInfo.fetchMaxBytes
    val tp = remoteStorageFetchInfo.topicPartition
    val fetchInfo: PartitionData = remoteStorageFetchInfo.fetchInfo

    val includeAbortedTxns = remoteStorageFetchInfo.fetchIsolation == FetchTxnCommitted

    val offset = fetchInfo.fetchOffset
    val maxBytes = Math.min(fetchMaxBytes, fetchInfo.maxBytes)

    val epoch = fetchLog(tp).flatMap(log => log.leaderEpochCache.flatMap(cache => cache.epochForOffset(offset)))
    val rlsMetadata = if (epoch.isDefined) {
      fetchRemoteLogSegmentMetadata(tp, epoch.get, offset)
    } else {
      Optional.empty()
    }

    if (!rlsMetadata.isPresent) {
      val epochStr = if (epoch.isDefined) epoch.get.toString else "NOT AVAILABLE"
      throw new OffsetOutOfRangeException(
        s"Received request for offset $offset for leader epoch $epochStr and partition $tp which does not exist in remote tier. Try again later.")
    }

    val startPos = lookupPositionForOffset(rlsMetadata.get(), offset)
    var remoteSegInputStream: InputStream = null
    try {
      // Search forward for the position of the last offset that is greater than or equal to the target offset
      remoteSegInputStream = remoteLogStorageManager.fetchLogSegment(rlsMetadata.get(), startPos)
      val remoteLogInputStream = new RemoteLogInputStream(remoteSegInputStream)

      val firstBatch = findFirstBatch(remoteLogInputStream, offset)

      if (firstBatch == null)
        return FetchDataInfo(LogOffsetMetadata(offset), MemoryRecords.EMPTY,
          abortedTransactions = if(includeAbortedTxns) Some(List.empty) else None)

      val updatedFetchSize =
        if (remoteStorageFetchInfo.minOneMessage && firstBatch.sizeInBytes() > maxBytes) firstBatch.sizeInBytes()
        else maxBytes

      val buffer = ByteBuffer.allocate(updatedFetchSize)
      var remainingBytes = updatedFetchSize

      firstBatch.writeTo(buffer)
      remainingBytes -= firstBatch.sizeInBytes()

      if(remainingBytes > 0) {
        // input stream is read till (startPos - 1) while getting the batch of records earlier.
        // read the input stream until min of (EOF stream or buffer's remaining capacity).
        Utils.readFully(remoteSegInputStream, buffer)
      }
      buffer.flip()

      var fetchDataInfo = FetchDataInfo(LogOffsetMetadata(offset), MemoryRecords.readableRecords(buffer))
      if (includeAbortedTxns) {
        fetchDataInfo = addAbortedTransactions(firstBatch.baseOffset(), rlsMetadata.get(), fetchDataInfo)
      }
      fetchDataInfo
    } finally {
      Utils.closeQuietly(remoteSegInputStream, "RemoteLogSegmentInputStream")
    }
  }

  private[remote] def findFirstBatch(remoteLogInputStream: RemoteLogInputStream, offset: Long): RecordBatch = {
    var nextBatch: RecordBatch = null

    def iterateNextBatch(): RecordBatch = {
      nextBatch = remoteLogInputStream.nextBatch()
      nextBatch
    }
    // Look for the batch which has the desired offset
    // we will always have a batch in that segment as it is a non-compacted topic. For compacted topics, we may need
    //to read from the subsequent segments if there is no batch available for the desired offset in the current
    //segment. That means, desired offset is more than last offset of the current segment and immediate available
    //offset exists in the next segment which can be higher than the desired offset.
    while (iterateNextBatch() != null && nextBatch.lastOffset < offset) {
    }
    nextBatch
  }

  private[remote] def addAbortedTransactions(startOffset: Long,
                                             segmentMetadata: RemoteLogSegmentMetadata,
                                             fetchInfo: FetchDataInfo): FetchDataInfo = {
    val fetchSize = fetchInfo.records.sizeInBytes
    val startOffsetPosition = OffsetPosition(fetchInfo.fetchOffsetMetadata.messageOffset,
      fetchInfo.fetchOffsetMetadata.relativePositionInSegment)

    val offsetIndex = indexCache.getIndexEntry(segmentMetadata).offsetIndex
    val upperBoundOffset = offsetIndex.fetchUpperBoundOffset(startOffsetPosition, fetchSize)
      .map(_.offset).getOrElse(segmentMetadata.endOffset()+1)

    val abortedTransactions = ListBuffer.empty[AbortedTransaction]
    def accumulator(abortedTxn: List[AbortedTxn]): Unit = abortedTransactions ++= abortedTxn.map(_.asAbortedTransaction)

    collectAbortedTransactions(startOffset, upperBoundOffset, segmentMetadata, accumulator)

    FetchDataInfo(fetchOffsetMetadata = fetchInfo.fetchOffsetMetadata,
      records = fetchInfo.records,
      firstEntryIncomplete = fetchInfo.firstEntryIncomplete,
      abortedTransactions = Some(abortedTransactions.toList))
  }

  private[remote] def collectAbortedTransactions(startOffset: Long,
                                                 upperBoundOffset: Long,
                                                 segmentMetadata: RemoteLogSegmentMetadata,
                                                 accumulator: List[AbortedTxn] => Unit): Unit = {
    val topicPartition = segmentMetadata.topicIdPartition().topicPartition()
    val localLogSegments = fetchLog(topicPartition).map(log => log.logSegments.iterator).getOrElse(Iterator.empty)

    var searchInLocalLog = false
    var nextSegmentMetadataOpt = Option.apply(segmentMetadata)
    var txnIndexOpt = nextSegmentMetadataOpt.map(metadata => indexCache.getIndexEntry(metadata).txnIndex)
    while (txnIndexOpt.isDefined) {
      val searchResult = txnIndexOpt.get.collectAbortedTxns(startOffset, upperBoundOffset)
      accumulator(searchResult.abortedTransactions)
      if (!searchResult.isComplete) {
        if (!searchInLocalLog) {
          nextSegmentMetadataOpt = nextSegmentMetadataOpt.flatMap(x => findNextSegmentMetadata(x))
          txnIndexOpt = nextSegmentMetadataOpt.map(x => indexCache.getIndexEntry(x).txnIndex)
          if (txnIndexOpt.isEmpty) {
            searchInLocalLog = true
          }
        }
        if (searchInLocalLog) {
          txnIndexOpt = if (localLogSegments.hasNext) Some(localLogSegments.next().txnIndex) else None
        }
      } else {
        return
      }
    }
  }

  private[remote] def findNextSegmentMetadata(segmentMetadata: RemoteLogSegmentMetadata): Option[RemoteLogSegmentMetadata] = {
    val topicPartition = segmentMetadata.topicIdPartition().topicPartition()
    val nextSegmentBaseOffset = segmentMetadata.endOffset()+1
    var epoch = Option(segmentMetadata.segmentLeaderEpochs().lastEntry().getKey.toInt)
    var result: Option[RemoteLogSegmentMetadata] = Option.empty;
    fetchLog(topicPartition).foreach ( log => {
      log.leaderEpochCache.foreach( cache => {
        while (result.isEmpty && epoch.isDefined) {
          result = Option(fetchRemoteLogSegmentMetadata(topicPartition, epoch.get, nextSegmentBaseOffset).orElse(null))
          epoch = cache.findNextEpoch(epoch.get)
        }
      })
    })
    result
  }

  def fetchRemoteLogSegmentMetadata(tp: TopicPartition,
                                    epochForOffset: Int,
                                    offset: Long): Optional[RemoteLogSegmentMetadata] = {
    val topicIdPartition =
      topicPartitionIds.get(tp) match {
        case Some(uuid) => Some(new TopicIdPartition(uuid, tp))
        case None => None
      }

    if (topicIdPartition.isEmpty) {
      throw new KafkaException("No topic id registered for topic partition: " + tp)
    }
    remoteLogMetadataManager.remoteLogSegmentMetadata(topicIdPartition.get, epochForOffset, offset)
  }

  def lookupTimestamp(rlsMetadata: RemoteLogSegmentMetadata, timestamp: Long, startingOffset: Long): Option[TimestampAndOffset] = {
    val startPos = indexCache.lookupTimestamp(rlsMetadata, timestamp, startingOffset)

    var remoteSegInputStream: InputStream = null
    try {
      // Search forward for the position of the last offset that is greater than or equal to the target offset
      remoteSegInputStream = remoteLogStorageManager.fetchLogSegment(rlsMetadata, startPos)
      val remoteLogInputStream = new RemoteLogInputStream(remoteSegInputStream)
      var batch: RecordBatch = null
      def nextBatch(): RecordBatch = {
        batch = remoteLogInputStream.nextBatch()
        batch
      }
      while (nextBatch() != null) {
        if (batch.maxTimestamp >= timestamp && batch.lastOffset >= startingOffset) {
          batch.iterator.asScala.foreach(record => {
            if (record.timestamp >= timestamp && record.offset >= startingOffset)
              return Some(new TimestampAndOffset(record.timestamp, record.offset, maybeLeaderEpoch(batch.partitionLeaderEpoch)))
          })
        }
      }
      None
    } finally {
      Utils.closeQuietly(remoteSegInputStream, "RemoteLogSegmentInputStream")
    }
  }

  private def maybeLeaderEpoch(leaderEpoch: Int): Optional[Integer] = {
    if (leaderEpoch == RecordBatch.NO_PARTITION_LEADER_EPOCH)
      Optional.empty()
    else
      Optional.of(leaderEpoch)
  }

  /**
   * Search the message offset in the remote storage based on timestamp and offset.
   *
   * This method returns an option of TimestampOffset. The returned value is determined using the following ordered list of rules:
   *
   * - If there is no messages in the remote storage, return None
   * - If all the messages in the remote storage have smaller offsets, return None
   * - If all the messages in the remote storage have smaller timestamps, return None
   * - If all the messages in the remote storage have larger timestamps, or no message in the remote storage has a timestamp
   * the returned offset will be max(the earliest offset in the remote storage, startingOffset) and the timestamp will
   * be Message.NoTimestamp.
   * - Otherwise, return an option of TimestampOffset. The offset is the offset of the first message whose timestamp
   * is greater than or equals to the target timestamp and whose offset is greater than or equals to the startingOffset.
   *
   * @param timestamp      The timestamp to search for.
   * @param startingOffset The starting offset to search.
   * @return the timestamp and offset of the first message that meets the requirements. None will be returned if there is no such message.
   */
  def findOffsetByTimestamp(tp: TopicPartition, timestamp: Long, startingOffset: Long,
                            leaderEpochCache: LeaderEpochFileCache): Option[TimestampAndOffset] = {
    val topicId = topicPartitionIds.get(tp)
    if (topicId.isEmpty) {
      throw new KafkaException("Topic id does not exist for topic partition: " + tp)
    }
    // Get the respective epoch in which the starting offset exists.
    var maybeEpoch = leaderEpochCache.epochForOffset(startingOffset)
    while (maybeEpoch.nonEmpty) {
      val epoch = maybeEpoch.get
      remoteLogMetadataManager.listRemoteLogSegments(new TopicIdPartition(topicId.get, tp), epoch).asScala
        .foreach(rlsMetadata =>
          if (rlsMetadata.maxTimestampMs() >= timestamp && rlsMetadata.endOffset() >= startingOffset) {
            val timestampOffset = lookupTimestamp(rlsMetadata, timestamp, startingOffset)
            if (timestampOffset.isDefined)
              return timestampOffset
          }
        )

      // Move to the next epoch if not found with the current epoch.
      maybeEpoch = leaderEpochCache.findNextEpoch(epoch)
    }
    None
  }
  /**
   * A remote log read task returned by asyncRead(). The caller of asyncRead() can use this object to cancel a
   * pending task or check if the task is done.
   */
  case class AsyncReadTask(future: Future[Unit]) {
    def cancel(mayInterruptIfRunning: Boolean): Boolean = {
      val r = future.cancel(mayInterruptIfRunning)
      if (r) {
        // Removed the cancelled task from task queue
        remoteStorageFetcherThreadPool.purge()
      }
      r
    }

    def isCancelled: Boolean = future.isCancelled

    def isDone: Boolean = future.isDone
  }

  /**
   * Submit a remote log read task.
   *
   * This method returns immediately. The read operation is executed in a thread pool.
   * The callback will be called when the task is done.
   *
   * @throws RejectedExecutionException if the task cannot be accepted for execution (task queue is full)
   */
  def asyncRead(fetchInfo: RemoteStorageFetchInfo, callback: RemoteLogReadResult => Unit): AsyncReadTask = {
    AsyncReadTask(remoteStorageFetcherThreadPool.submit(new RemoteLogReader(fetchInfo, this, brokerTopicStats, callback)))
  }

  /**
   * Stops all the threads and releases all the resources.
   */
  def close(): Unit = {
    if (closed)
      warn("Trying to close an already closed RemoteLogManager")
    else this synchronized {
      // Write lock is not taken when closing this class. As, the read lock is held by other threads which might be
      // waiting on the producer future (or) trying to consume the metadata record for strong consistency.
      if (!closed) {
        // During segment copy, the RLM task publishes an event and tries to consume the same for strong consistency.
        // The active RLM task might be waiting on the producer future (or) trying to consume the record.
        // So, tasks should be cancelled first, close the RLMM, RSM, then shutdown the thread pool to close the active
        // tasks.
        leaderOrFollowerTasks.values().forEach(_.cancel())
        Utils.closeQuietly(remoteLogMetadataManager, "RemoteLogMetadataManager")
        Utils.closeQuietly(remoteLogStorageManager, "RemoteLogStorageManager")
        rlmScheduledThreadPool.shutdown()
        remoteStorageFetcherThreadPool.shutdown()
        leaderOrFollowerTasks.clear()
        closed = true
      }
    }
  }

  case class RLMTaskWithFuture(rlmTask: RLMTask, future: Future[_]) {
    def cancel(): Unit = {
      rlmTask.cancel()
      try {
        future.cancel(true)
      } catch {
        case ex: Exception => error(s"Error occurred while canceling the task: $rlmTask", ex)
      }
    }
  }

  class InMemoryLeaderEpochCheckpoint extends LeaderEpochCheckpoint {
    private var epochs: Seq[EpochEntry] = Seq()
    override def write(epochs: Iterable[EpochEntry]): Unit = this.epochs = epochs.toSeq
    override def read(): Seq[EpochEntry] = this.epochs

    def readAsByteBuffer(): ByteBuffer = {
      val stream = new ByteArrayOutputStream()
      val writer = new BufferedWriter(new OutputStreamWriter(stream, StandardCharsets.UTF_8))
      val writeBuffer = new CheckpointWriteBuffer[EpochEntry](writer, version = 0, LeaderEpochCheckpointFile.Formatter)
      try {
        writeBuffer.write(epochs)
        writer.flush()
        ByteBuffer.wrap(stream.toByteArray)
      } finally {
        writer.close()
      }
    }
  }
}